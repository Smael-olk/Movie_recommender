# -*- coding: utf-8 -*-
"""Système de recommandation avec la bibliothèque surprise.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19HJW3mGszzbIvaCnvhZP-Nd1ZuxftCeS

# Recommender System

On construit un recommender system en utilisant  la bibliothèque surprise

[Système de recommandation](https://fr.wikipedia.org/wiki/Syst%C3%A8me_de_recommandation)  est une forme spécifique de filtrage de l'information (SI) visant à présenter les éléments d'information (films, musique, livres, news, images, pages Web, etc) qui sont susceptibles d'intéresser l'utilisateur.

Généralement, un système de recommandation permet de comparer le profil d'un utilisateur à certaines caractéristiques de référence, et cherche à prédire l'« avis » que donnerait un utilisateur. Ces caractéristiques peuvent provenir de  l'objet lui-même, on parle « d'approche basée sur le contenu » ou content-based approach ; l'utilisateur ; l'environnement social, on parle d'approche de filtrage collaboratif ou collaborative filtering.

## Installation des bibliothèques requises
"""

!pip3 install numpy
!pip3 install scikit-surprise



"""## Importation des bibliothèques """

import pandas as pd
import csv
from surprise import Reader
from collections import defaultdict
from surprise import Dataset,CoClustering
from surprise.model_selection import cross_validate
from surprise import KNNBasic
from surprise.accuracy import rmse, mae
from surprise import accuracy
from surprise.model_selection import train_test_split
from surprise.model_selection import GridSearchCV

"""## Importation du dataset"""

df = pd.read_csv ("u.data",sep="\t")

df.head()

df.tail()

df.head()

df.shape

df.info()

"""### Dimensionalité

On reduit la dimensionalité du dataset, on éliminant les films peu évalués et les utilisateurs peu évaluants.
"""

min_ratings = 5
filter_items = df['movie id'].value_counts() > min_ratings
filter_items = filter_items[filter_items].index.tolist()

min_user_ratings = 5
filter_users = df['user id'].value_counts() > min_user_ratings
filter_users = filter_users[filter_users].index.tolist()

df_new = df[(df['movie id'].isin(filter_items)) & (df['movie id'].isin(filter_users))]
print('The original data frame shape:\t{}'.format(df.shape))
print('The new data frame shape:\t{}'.format(df_new.shape))

"""## Surprise

On convertit les données au format requise par Surprise
"""

reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(df_new[['user id', 'movie id', 'rating']], reader)
# Division en dataset train et test
train_set, test_set = train_test_split(data, test_size=.2)

"""### k-NN algorithms

#### KNNBasic

* KNNBasic "K-nearest neighbours" est un algorithme de filtrage collaboratif.
On utilise içi pour les similarités :
  Cosine
  Mean square difference
  Pearson

On utilise le rmse pour la mesure de la précision de nos prédictions

#Cross-validation de chaque algorithme en utilisant Surprise
"""

def precision_recall_calculation(predictions, threshold=3.5):
    # Associe chaque prédiction à son utilisateur
    user_predict_true = defaultdict(list)
    for user_id, movie_id, true_rating, predicted_rating, _ in predictions:
        user_predict_true[user_id].append((predicted_rating, true_rating))

    precisions = dict()
    recalls = dict()
    for user_id, user_ratings in user_predict_true.items():
        # ordonner les utilisateurs en utilisant la valeur estimée
        user_ratings.sort(key=lambda x: x[0], reverse=True)
        # Nombre des films pertinents
        no_of_relevant_items = sum((true_rating >= threshold) for (predicted_rating, true_rating) in user_ratings)
        # Nombre des films recommandés dans le top 10
        no_of_recommended_items = sum((predicted_rating >= threshold) for (predicted_rating, true_rating) in user_ratings[:10])
        # Nomnbre des films recommandés et pertinents dans le top 10
        no_of_relevant_and_recommended_items = sum(((true_rating >= threshold) and (predicted_rating >= threshold)) for (predicted_rating, true_rating) in user_ratings[:10])
        # Precision: Proportion des films recommandées pertinents
        precisions[user_id] = no_of_relevant_and_recommended_items / no_of_recommended_items if no_of_recommended_items != 0 else 1
        # Recall: Proportions des films pertinents qui ont été recommandé
        recalls[user_id] = no_of_relevant_and_recommended_items / no_of_relevant_items if no_of_relevant_items != 0 else 1

    # Moyennant pour tous les utilisateurs
    average_precision=sum(precision for precision in precisions.values()) / len(precisions)
    average_recall=sum(recall for recall in recalls.values()) / len(recalls)
    F_score=(2*average_precision*average_recall) / (average_precision + average_recall)
    
    return [average_precision, average_recall, F_score]

"""En utilisant des similarités différents on applique la fonction de validation de l'algorithme pour donner enfin le rmse de l'algorithme K-plus proche voisin avec les similarités :


*   Cosine
*   MSD (Mean square difference)
*   Pearson coefficient


"""

sim_options = {
    "name": "cosine",
    "user_based": True,  # compute  similarities between users using cosine similarity
}
sim_options1 = {
    "name": "msd",
    "user_based": False,  # compute  similarities between items using msd similarity
}
sim_options2 = {
    "name": "pearson",
    "user_based": True,  # compute  similarities between users using using pearson correlation coefficient
}
rows = []
algorithms = [KNNBasic(sim_options= sim_options),KNNBasic( sim_options = sim_options1 ),KNNBasic(
sim_options= sim_options2)]
for algorithm in algorithms:
    avg_dict = {}
    algorithm_name = str(algorithm).split(' ')[0].split('.')[-1]
    results = cross_validate(algorithm, data, measures=['RMSE', 'MAE'], cv=10, verbose=False)
    for k, v in results.items(): avg_dict[k] = sum(v)/ float(len(v))
#     algorithm.fit(train_set)
#     predictions = algorithm.test(test_set)
#     precision, recall, F_score = precision_recall_calculation(predictions, threshold=3.5)
    rows.append([ avg_dict['test_rmse'], avg_dict['test_mae'], avg_dict['fit_time'], avg_dict['test_time']])

benchmarks_df = pd.DataFrame(rows, columns=[ 'test_rmse', 'test_mae', 'fit_time', 'test_time'])
names=["KNNBasic with cosine","KNNBasic with MSD","KNNBasic with pearson"]
benchmarks_df["Algorithm"]=names
benchmarks_df.set_index("Algorithm",inplace=True)
display(benchmarks_df)

"""#Utilisation de GridsearchCV pour trouver les paramètres optimales en se basant sur le Dataset d'entrainement afin de donner le meilleur RMSE et MAE avec les paramètres correspondants."""

# param_grid = {
#     "n_epochs": [5, 10, 15, 20, 30, 40, 50, 100],
#     "lr_all": [0.001, 0.002, 0.005],
#     "reg_all": [0.02, 0.08, 0.4, 0.6]
# }

# smaller grid for testing
param_grid = {
    "n_epochs": [10, 20],
    "lr_all": [0.002, 0.005],
    "reg_all": [0.02]
}
class KNNBasic_cosine(KNNBasic):
    def __init__(self, *args, sim_options={"name":"cosine","user_based":True}, **kwargs):
        super().__init__(*args, sim_options={"name":"cosine","user_based":True}, **kwargs)

class KNNBasic_pearson(KNNBasic):
    def __init__(self, *args, sim_options={"name":"pearson","user_based":True}, **kwargs):
        super().__init__(*args, sim_options={"name":"pearson","user_based":True}, **kwargs)

gs = GridSearchCV(KNNBasic_cosine, param_grid, measures=["rmse", "mae"], refit=True, cv=5)

gs.fit(data)

training_parameters = gs.best_params["rmse"]

print("BEST RMSE: \t", gs.best_score["rmse"])
print("BEST MAE: \t", gs.best_score["mae"])
print("BEST params: \t", gs.best_params["rmse"])

"""#Prediction des évaluations avec  CoClustring pour comparer la précision """

COC=KNNBasic_cosine(n_epochs=training_parameters['n_epochs'])
COC.fit(train_set)

# Predict on the test dataset
test_set_predicted = COC.test(test_set)
print("Deviation RMSE: {}".format(rmse(test_set_predicted, verbose=False)))
print("Deviation MAE: \t{}".format(mae(test_set_predicted, verbose=False)))
precision, recall, F_score = precision_recall_calculation(test_set_predicted, threshold=3.5)
print("Precision: \t{}".format(precision))
print("Recall: \t{}".format(recall))
print("F_score: \t{}".format(F_score))

"""#Génération des évaluations estimés à coté des évaluations réelles et calcul de précision"""

result = pd.DataFrame(test_set_predicted, columns=['UserId', 'MovieId', 'Rating', 'est', 'details'])
result['est'] = result['est'].round(decimals=1)
result['est'] = result['est'].apply(lambda x: round(0.5 *round(x/0.5), 2))
result['error'] = abs(result.est - result.Rating)
display(result.sort_values(by='error', ascending=True))

print("Accuracy = {} %".format(100-round((result[result['error']>=1].shape[0]/result.shape[0]),2)))

"""#Etablissement du programme de recommendation """

def get_top_n(predictions, n=10):
    # Association des prédictions aux utilisateurs
    top_n = defaultdict(list)
    for uid, iid, true_r, est, _ in predictions:
        top_n[uid].append((iid, est))

    # tri des prédictions et retour des top-k
    for uid, user_ratings in top_n.items():
        user_ratings.sort(key=lambda x: x[1], reverse=True)
        top_n[uid] = user_ratings[:n]

    return top_n

movieID_to_name = {}
with open('movies.csv', newline='', encoding='ISO-8859-1') as csvfile:
        movie_reader = csv.reader(csvfile)
        next(movie_reader)
        for row in movie_reader:
            movieID = int(row[0])
            movie_name = row[1]
            movieID_to_name[movieID] = movie_name
                
def getMovieName(movieID):
    if int(movieID) in movieID_to_name:
        return movieID_to_name[int(movieID)]
    else:
        return ""

top_n = get_top_n(test_set_predicted, n=10)
# pd.set_option('display.max_columns', None)  # or 1000
# pd.set_option('display.max_rows', None)  # or 1000
# pd.set_option('display.max_colwidth', None)  # or 199
ff=pd.DataFrame(columns=['User', 'Recommendations'])
# Print the recommended items for each user
for uid, user_ratings in top_n.items():
    if len(user_ratings)>=10:
        ff = ff.append({'User': uid, 'Recommendations': ', '.join([getMovieName(iid) for (iid, _) in user_ratings])}, ignore_index=True)
display(ff)